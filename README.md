# SPOTS10-KD-Benchmark

A benchmark and implementation of modern knowledge distillation methods on the SPOT-10 animal pattern dataset, including Logit Standardization, TTM, and WTTM.

This repository provides a clean benchmark for evaluating modern knowledge distillation (KD) techniques on the SPOT-10 dataset â€” a challenging greyscale animal pattern recognition dataset designed for low-light conditions.

We implement and compare the following KD methods:
- **Classical Knowledge Distillation** (Hinton et al.)
- **Logit Standardization (LS)**
- **Transformed Teacher Matching (TTM)**
- **Weighted TTM (WTTM)**

## ğŸ” Motivation
While the original SPOT-10 paper used only classical KD, we extend its utility by introducing and evaluating more advanced distillation methods. This allows us to explore their effectiveness on low-contrast, texture-based tasks in a compact but realistic setting.

## ğŸ“ˆ Contributions
- ğŸ“¦ First application of LS, TTM, and WTTM on SPOT-10
- ğŸ§ª Unified training and evaluation pipeline for reproducible experiments
- ğŸ“Š Performance comparison (validation & per-class accuracy)
- ğŸ“ Per-sample disagreement analysis between distillation methods

## ğŸ—‚ Directory Structure
